# -*- coding: utf-8 -*-
"""generative-exp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QtPIQQ2xtBVgW6ophtW0xB1wNk8mkaBd

# environment config
"""
#
# !pip install transformers
# !pip install jsonlines
# !pip install wandb

import torch
import argparse

print(torch.cuda.is_available())
device_id = 0 if torch.cuda.is_available() else 'cpu'  # Equivalent to device_id = 'cuda:0'
device = torch.device(device_id)  # use these semantics to specify a specific device.
#
# from google.colab import drive
# drive.mount('/content/drive')
#
# import os
# os.chdir('/content/drive/MyDrive/263/')

import wandb

wandb.login()

"""# dataset """

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold
from torch.utils.data.sampler import SequentialSampler

import json

PLACE_HOLDER = '@placeholder'


def json2tuple(json_instance):
    article = json_instance["article"]
    question = json_instance["question"]
    opt0, opt1, opt2, opt3, opt4 = json_instance["option_0"], json_instance["option_1"], \
        json_instance["option_2"], json_instance["option_3"], json_instance["option_4"]
    opts = [opt0, opt1, opt2, opt3, opt4]
    # answer = opts[json_instance["label"]]
    label = json_instance["label"]
    # print(opts)
    instance_tuple = (article, question, opts, label)

    return instance_tuple


def transform(instance):
    article, question, opts, label = instance
    question = question.replace(PLACE_HOLDER, '<mask>')
    query = question + '</s></s>' + article
    return query, opts, label


class ClozeDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_len=512):
        self.data = []
        self.mask_index = []
        self.opts = []
        self.label = []
        self.tokenizer = tokenizer
        self.max_len = max_len

        # 从JSONL文件中读取数据
        with open(file_path, 'r') as file:
            for line in file:
                json_obj = json.loads(line)
                instance_tuple = json2tuple(json_obj)
                query, opts, label = transform(instance_tuple)
                self.data.append(query)
                self.opts.append(opts)
                self.label.append(label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text = self.data[index]
        opts = self.opts[index]
        label = self.label[index]

        text_encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,  # 根据需求设置最大长度
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        # mask_token_index = torch.where(text_encoding == tokenizer.mask_token_id)[1]
        opts_token_id = [self.tokenizer.encode(opt, add_special_tokens=False)[0] for opt in opts]
        # answer_token_id = tokenizer.encode(answer, add_special_tokens=False)[0]
        answer_token_id = opts_token_id[label]
        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, padding='max_length',
                                            truncation=True, max_length=self.max_len)
        input_ids = inputs['input_ids']

        mask_index = input_ids.index(self.tokenizer.mask_token_id)
        # print(opts)
        # print('opts',opts_token_id)
        # # print(answer)
        # print('answer', answer_token_id.type)
        # print('opttoken',opts_token_id.type)
        # print(opts_token_id)
        opts_tensor = torch.tensor(opts_token_id)

        return {
            'input_ids': text_encoding['input_ids'].squeeze(),
            'attention_mask': text_encoding['attention_mask'].squeeze()
        }, opts_tensor, answer_token_id, mask_index


"""# training"""

import torch
from transformers import RobertaTokenizer, RobertaForMaskedLM
from torch.optim import AdamW
from torch.utils.data import DataLoader
import torch.nn as nn
from sklearn.metrics import accuracy_score
import numpy as np



def correct_num(logits, mask_indices, opts, answers):
    pred_masked_probs = logits[torch.arange(logits.size(0)), mask_indices]
    opts = opts.to(pred_masked_probs.device)
    # the probs of the whole vocab at the masked position
    opts_probs = pred_masked_probs.gather(dim=1, index=opts)
    pred_answers_index = torch.argmax(opts_probs, dim=1)
    pred_answers = opts.gather(dim=1, index=pred_answers_index.unsqueeze(1)).squeeze(dim=1)
    correct_samples = torch.eq(answers, pred_answers).sum().item()
    return correct_samples
def get_newdata_path(task_id):
    if task_id == 0:
        train_json_path = ("../data/training_data/train_test.jsonl")
        val_json_path = ("../data/training_data/train_test.jsonl")
        test_json_path = ("../data/training_data/train_test.jsonl")
        test_another_json_path = ("../data/training_data/train_test.jsonl")
    elif task_id == 1:
        train_json_path = ('../newdata/train/Task_1_train.jsonl')
        val_json_path = ('../newdata/val/Task_1_val.jsonl')
        test_json_path = ('../newdata/test/Task_1_test.jsonl')
        test_another_json_path = ('../newdata/test/Task_2_test.jsonl')

    elif task_id == 2:
        train_json_path = ('../newdata/train/Task_2_train.jsonl')
        val_json_path = ('../newdata/val/Task_2_val.jsonl')
        test_json_path = ('../newdata/test/Task_2_test.jsonl')
        test_another_json_path = ('../newdata/test/Task_1_test.jsonl')
    else:
        print('Wrong task id, task id should be 1, 2')
        return
    return train_json_path,val_json_path, test_json_path, test_another_json_path

def get_data_path(task_id):
    if task_id == 0:
        train_val_json_path = ("../data/training_data/train_test.jsonl")
        test_json_path = ("../data/training_data/train_test.jsonl")
    elif task_id == 1:
        train_val_json_path = ("../data/training_data/Task_1_train.jsonl")
        test_json_path = ("../data/training_data/Task_1_dev.jsonl")

    elif task_id == 2:
        train_val_json_path = ("../data/training_data/Task_2_train.jsonl")
        test_json_path = ("../data/training_data/Task_2_dev.jsonl")

    elif task_id == 3:
        train_val_json_path = ("../data/training_data/Task_1_train.jsonl")
        test_json_path = ("../data/training_data/Task_2_dev.jsonl")

    elif task_id == 4:
        train_val_json_path = ("../data/training_data/Task_2_train.jsonl")
        test_json_path = ("../data/training_data/Task_1_dev.jsonl")
    else:
        print('Wrong task id, task id should be 1, 2, 3')
        return
    return train_val_json_path, test_json_path

def evaluate(model, train_loader, device, k=5):
    model.to(device)
    model.eval()  # set model to training mode.

    total_loss = 0
    y_true = []
    y_pred = []
    sample_num = 0
    correct_topk = 0
    correct = 0

    for inputs, opts, answers, mask_indices in train_loader:
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        # opts = opts.to(device)
        answers = answers.to(device)
        mask_indices = mask_indices.to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        batch_logits = logits[torch.arange(len(logits)), mask_indices]

        correct += correct_num(logits, mask_indices, opts, answers)
        sample_num += len(answers)

    acc = correct / sample_num

    return acc

def train(model, train_loader, optimizer, loss_fn, device, k=5):
    model.to(device)
    model.train()  # set model to training mode.

    total_loss = 0
    sample_num = 0
    correct_topk = 0
    correct = 0

    for inputs, opts, answers, mask_indices in train_loader:


        optimizer.zero_grad()
        input_ids = inputs["input_ids"].to(device)

        attention_mask = inputs["attention_mask"].to(device)
        answers = answers.to(device)
        mask_indices = mask_indices.to(device)
        opts = opts.to(device)

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        correct +=correct_num(logits, mask_indices, opts, answers)


        batch_logits = logits[torch.arange(len(logits)), mask_indices]
        # Compute the loss
        loss = loss_fn(batch_logits, answers)
        total_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        # print(answers)

        _, top_k_indices = torch.topk(batch_logits, k, dim=1)  # 获取预测结果中的 Top-k 索引

        sample_num += len(answers)

        if sample_num % 500 == 0:
            print(str(sample_num) + "samples, Loss:" + str(loss))

        # Calculate metrics for the epoch
    # topk_acc = correct_topk / sample_num
    avg_loss = total_loss / len(train_loader)
    acc = correct / sample_num
    # return topk_acc, avg_loss, correctness
    return acc, avg_loss

def train_model(model, train_loader, val_loader, test_loader, another_test_loader, optimizer, loss_fn, device, num_epochs=10):
    # best_val_accuracy = 0.0
    best_val_acc=0.0
    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        print("-" * 10)
        # Training
        train_acc,train_loss = train(model, train_loader, optimizer, loss_fn, device)
        print(
            f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        wandb.log({"Train Loss": train_loss, "Train Acc": train_acc})

        # Validation
        val_acc = evaluate(model, val_loader, device)
        print(f"Validation Accuracy: {val_acc:.4f} ")
        wandb.log({"Val Acc": val_acc})

        # Check if the current model has the best validation accuracy
        # if val_accuracy > best_val_accuracy:
        #     best_val_accuracy = val_accuracy
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_generative_model.pt")
            print("Best model saved!")

        print()

    print("Training complete.")

    # Load the best model and evaluate on the test set
    model.load_state_dict(torch.load("best_generative_model.pt"))
    test_acc= evaluate(model, test_loader, device)
    wandb.log({"Test Acc": test_acc})
    print(f"Test Accuracy: {test_acc:.4f}")
    # print(f"Test Accuracy: {test_accuracy:.4f}")

    # model.load_state_dict(torch.load("best_generative_model.pt"))
    test_acc_2 = evaluate(model, another_test_loader, device)
    wandb.log({"Test Acc 2": test_acc_2})
    print(f"Test Accuracy 2: {test_acc_2:.4f}")


def kfold_tune(k=5, train_batch_size=50, learning_rate=1e-5, num_epochs=10, checkpoint="roberta-base", max_len=512,
               task_id=1):
    # ############### loading data

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print('device:', device)
    tokenizer = RobertaTokenizer.from_pretrained(checkpoint)
    model = RobertaForMaskedLM.from_pretrained(checkpoint)

    train_val_json_path, test_json_path= get_data_path(task_id)

    train_val_dataset = ClozeDataset(train_val_json_path, tokenizer, max_len=max_len)

    test_dataset = ClozeDataset(test_json_path, tokenizer, max_len=max_len)
    test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)

    # ------------k fold---------------------

    kfold = KFold(n_splits=k, shuffle=True)

    for fold, (train_index, val_index) in enumerate(kfold.split(train_val_dataset)):
        print('Fold ', fold)

        train_sampler = SequentialSampler(train_index)
        val_sampler = SequentialSampler(val_index)
        train_loader = DataLoader(train_val_dataset, batch_size=train_batch_size, sampler=train_sampler)
        val_loader = DataLoader(train_val_dataset, batch_size=5, sampler=val_sampler)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        optimizer = AdamW(model.parameters(), lr=learning_rate)
        loss_fn = nn.CrossEntropyLoss()

        train_model(model, train_loader, val_loader, test_loader, optimizer, loss_fn, device, num_epochs=num_epochs)


def tune( train_batch_size=50, learning_rate=1e-5, num_epochs=10, checkpoint="roberta-base", max_len=512,
               task_id=1):
    # ############### loading data

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print('device:', device)
    tokenizer = RobertaTokenizer.from_pretrained(checkpoint)
    model = RobertaForMaskedLM.from_pretrained(checkpoint)

    train_json_path, val_json_path, test_json_path , test_another_json_path = get_newdata_path(task_id)

    train_set = ClozeDataset(train_json_path, tokenizer, max_len=max_len)
    val_set = ClozeDataset(val_json_path, tokenizer, max_len=max_len)
    test_set = ClozeDataset(test_json_path, tokenizer, max_len=max_len)
    another_test_set = ClozeDataset(test_another_json_path, tokenizer, max_len=max_len)

    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=False)
    val_loader = DataLoader(val_set, batch_size=5, shuffle=False)
    test_loader = DataLoader(test_set, batch_size=5, shuffle=False)
    another_test_loader = DataLoader(another_test_set, batch_size=5, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    optimizer = AdamW(model.parameters(), lr=learning_rate)
    loss_fn = nn.CrossEntropyLoss()

    train_model(model, train_loader, val_loader, test_loader, another_test_loader, optimizer, loss_fn, device, num_epochs=num_epochs)


"""# main"""

if __name__ == '__main__':
    wandb.login()
    parser = argparse.ArgumentParser(description='Hyper Prm Setting')
    parser.add_argument("-b", "--batch_size", type=int, default=50)
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-5)
    parser.add_argument("-e", "--num_epochs", type=int, default=10)
    parser.add_argument("-c", "--checkpoint", type=str, default="roberta-base")
    parser.add_argument("-l", "--max_len", type=int, default=512)
    parser.add_argument("-k", "--kfold", type=int, default=5)
    parser.add_argument("-t", "--task", type=int, default=1)

    # parser.add_argument("-b", "--batchsize", type=int)

    args = parser.parse_args()
    train_batch_size, learning_rate, num_epochs, checkpoint, max_len, k, task_id = \
        args.batch_size, args.learning_rate, args.num_epochs, args.checkpoint, args.max_len, args.kfold, args.task

    # ######################### hyper prm setting
    run = wandb.init(
        # settings=wandb.Settings(start_method="fork"),
        settings=wandb.Settings(start_method="thread"),
        # Set the project where this run will be logged
        # project="roberta_generative_task" + str(task_id),
        project="test" ,
        # Track hyperparameters and run metadata
        config={
            "learning_rate": learning_rate,
            "epochs": num_epochs,
            "batch_size": train_batch_size,
            "max_len": max_len,
            "checkpoint": checkpoint,
            "kfold": k

        })
    #
    # ####################
    # kfold_tune(k, train_batch_size, learning_rate, num_epochs, checkpoint, max_len, task_id)
    tune(train_batch_size, learning_rate, num_epochs, checkpoint, max_len, task_id)
