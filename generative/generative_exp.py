# -*- coding: utf-8 -*-
"""generative-exp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QtPIQQ2xtBVgW6ophtW0xB1wNk8mkaBd

# environment config
"""
#
# !pip install transformers
# !pip install jsonlines
# !pip install wandb

import torch
print(torch.cuda.is_available())
device_id = 0 if torch.cuda.is_available() else 'cpu' # Equivalent to device_id = 'cuda:0'
device = torch.device(device_id) # use these semantics to specify a specific device.
#
# from google.colab import drive
# drive.mount('/content/drive')
#
# import os
# os.chdir('/content/drive/MyDrive/263/')

import wandb
wandb.login()

"""# dataset """

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from torch.utils.data import Dataset, DataLoader

import json
PLACE_HOLDER = '@placeholder'
def json2tuple(json_instance):

    article = json_instance["article"]
    question = json_instance["question"]
    opt0, opt1, opt2, opt3, opt4 = json_instance["option_0"], json_instance["option_1"], \
            json_instance["option_2"], json_instance["option_3"], json_instance["option_4"]
    opts = [opt0, opt1, opt2, opt3, opt4]
    # answer = opts[json_instance["label"]]
    label = json_instance["label"]
    # print(opts)
    instance_tuple = (article, question, opts, label)

    return instance_tuple

def transform(instance):
    article, question, opts, label = instance
    question = question.replace(PLACE_HOLDER, '<mask>')
    query = question + '</s></s>' +article
    return query, opts, label
class ClozeDataset(Dataset):
    def __init__(self, file_path, tokenizer,max_len=512):
        self.data = []
        self.mask_index = []
        self.opts = []
        self.label = []
        self.tokenizer = tokenizer
        self.max_len = max_len

        # 从JSONL文件中读取数据
        with open(file_path, 'r') as file:
            for line in file:
                json_obj = json.loads(line)
                instance_tuple = json2tuple(json_obj)
                query, opts, label = transform(instance_tuple)
                self.data.append(query)
                self.opts.append(opts)
                self.label.append(label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text = self.data[index]
        opts = self.opts[index]
        label = self.label[index]

        text_encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,  # 根据需求设置最大长度
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        # mask_token_index = torch.where(text_encoding == tokenizer.mask_token_id)[1]
        opts_token_id = [self.tokenizer.encode(opt, add_special_tokens=False)[0] for opt in opts]
        # answer_token_id = tokenizer.encode(answer, add_special_tokens=False)[0]
        answer_token_id = opts_token_id[label]
        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, padding='max_length',
                                            truncation=True, max_length=self.max_len)
        input_ids = inputs['input_ids']

        mask_index =input_ids.index(self.tokenizer.mask_token_id)
        # print(opts)
        # print('opts',opts_token_id)
        # # print(answer)
        # print('answer', answer_token_id)

        return {
            'input_ids': text_encoding['input_ids'].squeeze(),
            'attention_mask': text_encoding['attention_mask'].squeeze()
        }, opts_token_id, answer_token_id, mask_index

"""# training"""

import torch
from transformers import RobertaTokenizer,RobertaForMaskedLM
from torch.optim import AdamW
from torch.utils.data import DataLoader
import torch.nn as nn
from sklearn.metrics import accuracy_score
import numpy as np


def evaluate(model, train_loader,device, k=5):
    model.to(device)
    model.eval()  # set model to training mode.

    total_loss = 0
    y_true = []
    y_pred = []
    sample_num = 0
    correct_topk = 0
    correct_num=0

    for inputs, opts, answers, mask_indices in train_loader:

        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        # opts = opts.to(device)
        answers = answers.to(device)
        mask_indices = mask_indices.to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        batch_logits = logits[torch.arange(len(logits)), mask_indices]


        _, top_k_indices = torch.topk(batch_logits, k, dim=1)  # 获取预测结果中的 Top-k 索引
        _, top_1_indices = torch.topk(batch_logits, 1, dim=1)  # 获取预测结果中的 Top-k 索引
        # print(top_k_indices)
        correct = torch.sum(top_1_indices == answers.view(-1, 1), dim=1)
        correct_num += torch.sum(correct > 0).item()
        correct_k = torch.sum(top_k_indices == answers.view(-1, 1), dim=1)  # 比较 Top-k 索引与真实标签
        correct_topk += torch.sum(correct_k > 0).item()
        sample_num+=len(answers)


        # correct = torch.sum(top_k_indices == answers.view(-1, 1), dim=1)  # 比较 Top-k 索引与真实标签
        # correct_topk += torch.sum(correct > 0).item()

        # Calculate metrics for the epoch
    # accuracy = accuracy_score(y_true, y_pred)
    topk_acc = correct_topk / sample_num
    correctness = correct_num / len(train_loader)
    return topk_acc, correctness
def train(model, train_loader, optimizer, loss_fn, device,k=5):
    model.to(device)
    model.train()  # set model to training mode.

    total_loss = 0
    y_true = []
    y_pred = []
    sample_num=0
    correct_topk=0

    for inputs, opts, answers, mask_indices in train_loader:
        # inputs = inputs.to(device)
        # labels = labels.to(device)
        optimizer.zero_grad()
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        answers = answers.to(device)
        mask_indices = mask_indices.to(device)

        # Forward pass
        # outputs = model(inputs)
        outputs = model(input_ids, attention_mask=attention_mask)
        # logits = outputs.logits.squeeze()
        logits = outputs.logits
        # print(logits)
        # print(logits.shape)
        batch_logits = logits[torch.arange(len(logits)), mask_indices]
        # Compute the loss
        # print(batch_logits.shape)

        loss = loss_fn(batch_logits,answers)
        total_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        # print(answers)

        _, top_k_indices = torch.topk(batch_logits, k, dim=1)  # 获取预测结果中的 Top-k 索引
        correct = torch.sum(top_k_indices == answers.view(-1, 1), dim=1)  # 比较 Top-k 索引与真实标签
        # print(correct)

        correct_topk += torch.sum(correct > 0).item()
        sample_num+=len(answers)

        if sample_num%500==0:
            print(str(sample_num)+"samples, Loss:" + str(loss))


        # Calculate metrics for the epoch
    # accuracy = accuracy_score(y_true, y_pred)
    # print('samplenum',sample_num)
    # print('len',len(train_loader))
    topk_acc = correct_topk/sample_num
    avg_loss = total_loss / len(train_loader)
    return topk_acc, avg_loss

def train_model(model, train_loader, val_loader, test_loader, optimizer, loss_fn, device, num_epochs=10):
    # best_val_accuracy = 0.0
    best_val_corr=0.0
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        print("-" * 10)
        # Training
        train_accuracy, train_loss = train(model, train_loader, optimizer, loss_fn, device)
        print(f"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}")
        wandb.log({"Train Loss": train_loss, "Train Acc": train_accuracy})

        # Validation
        val_accuracy, val_corr = evaluate(model, val_loader, device)
        print(f"Validation Accuracy: {val_accuracy:.4f} | Val Correctness:{val_corr:.4f} ")
        wandb.log({"Val Acc": val_accuracy, "Val Correctness": val_corr})

        # Check if the current model has the best validation accuracy
        # if val_accuracy > best_val_accuracy:
        #     best_val_accuracy = val_accuracy
        if val_corr > best_val_corr:
            best_val_corr = val_corr
            torch.save(model.state_dict(), "best_generative_model.pt")
            print("Best model saved!")

        print()

    print("Training complete.")

    # Load the best model and evaluate on the test set
    model.load_state_dict(torch.load("best_generative_model.pt"))
    test_accuracy, test_corr = evaluate(model, test_loader, device)
    wandb.log({"Test Acc": test_accuracy, "Test Correctness": test_corr})
    # print(f"Test Accuracy: {test_accuracy:.4f}")

    print(f"Test Accuracy: {test_accuracy:.4f} | Test Correctness:{test_corr:.4f}")



def tune(train_batch_size=50, learning_rate=1e-5, num_epochs=10, checkpoint="roberta-base", max_len=512):
    # ############### loading data
    # train_json_path = ("./data/training_data/train_test.jsonl")
    # val_json_path = ("./data/training_data/train_test.jsonl")
    # test_json_path = ("./data/training_data/train_test.jsonl")

    train_json_path = ("../data/training_data/Task_1_train.jsonl")
    val_json_path = ("../data/training_data/Task_1_dev.jsonl")
    test_json_path = ("../data/trail_data/Task_1_Imperceptibility.jsonl")

    train_binary_dataset = ClozeDataset(train_json_path, tokenizer, max_len=max_len)
    train_loader = DataLoader(train_binary_dataset, batch_size=train_batch_size, shuffle=False)

    val_binary_dataset = ClozeDataset(val_json_path, tokenizer, max_len=max_len)
    val_loader = DataLoader(val_binary_dataset, batch_size=5, shuffle=False)

    test_binary_dataset = ClozeDataset(test_json_path, tokenizer, max_len=max_len)
    test_loader = DataLoader(test_binary_dataset, batch_size=5, shuffle=False)

    # ######################### hyper prm setting
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    optimizer = AdamW(model.parameters(), lr=learning_rate)
    loss_fn = nn.CrossEntropyLoss()

    train_model(model, train_loader, val_loader, test_loader, optimizer, loss_fn, device, num_epochs=num_epochs)


"""# main"""

if __name__ == '__main__':
    wandb.login()
    parser = argparse.ArgumentParser(description='Hyper Prm Setting')
    parser.add_argument("-b", "--batch_size", type=int, default=50)
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-5)
    parser.add_argument("-e", "--num_epochs", type=int, default=10)
    parser.add_argument("-c", "--checkpoint", type=str, default="roberta-base")
    parser.add_argument("-l", "--max_len", type=int, default=512)

    # parser.add_argument("-b", "--batchsize", type=int)

    args = parser.parse_args()
    train_batch_size, learning_rate, num_epochs, checkpoint, max_len = \
        args.batch_size, args.learning_rate, args.num_epochs, args.checkpoint, args.max_len


    # checkpoint = "roberta-base"
    # tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
    # model = RobertaForMaskedLM.from_pretrained('roberta-base')
    # train_batch_size = 50
    # learning_rate = 1e-5
    # num_epochs = 10
    # max_len=512

    # ######################### hyper prm setting
    run = wandb.init(
        # settings=wandb.Settings(start_method="fork"),
        settings=wandb.Settings(start_method="thread"),
        # Set the project where this run will be logged
        project="roberta_generative",
        # Track hyperparameters and run metadata
        config={
            "learning_rate": learning_rate,
            "epochs": num_epochs,
            "batch_size": train_batch_size,
            "max_len": max_len,
            "checkpoint": checkpoint

        })
    #
    # ####################
    tune(train_batch_size, learning_rate, num_epochs, checkpoint, max_len)
