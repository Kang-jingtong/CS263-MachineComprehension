# -*- coding: utf-8 -*-
"""discriminate-exp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AaEDcmw_YedG6r-hqn0tEb7YyMGkhnwG

# environment config
"""

import torch
import wandb
import argparse
from torch.utils.data.sampler import SequentialSampler

"""# dataset """

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold
import json

PLACE_HOLDER = '@placeholder'


def substitute(question, opt):
    question = question.replace(PLACE_HOLDER, opt)
    return question


def json_query_complete(json_instance):
    article = json_instance["article"]
    question = json_instance["question"]
    opt0, opt1, opt2, opt3, opt4 = json_instance["option_0"], json_instance["option_1"], \
        json_instance["option_2"], json_instance["option_3"], json_instance["option_4"]
    opts = [opt0, opt1, opt2, opt3, opt4]
    label = json_instance["label"]
    instance_tuple = (article, question, opts, label)

    return instance_tuple


def transform_binary(instance, new_label, new_query):
    article, question, opts, label = instance

    # scores = [0.0] * len(opts)
    # scores[label] = 1.0
    questions = [substitute(question, opt) for opt in opts]

    for i in range(len(questions)):
        Q_i = questions[i]
        seq_i = Q_i + '</s></s>' + article
        # seq_i = Q_i + '[SEP]' + article
        label_i = 1 if label == i else 0
        new_label.append(label_i)
        new_query.append(seq_i)


# def json2data(file_path):
#     data = []
#     label = []
#     with open(file_path, 'r') as file:
#         for line in file:
#             new_label = []
#             new_query = []
#             json_obj = json.loads(line)
#             json_instance = json_query_complete(json_obj)
#             transform_binary(json_instance, new_label, new_query)
#             # print(len(new_query))
#             # print(new_label)
#             data.extend(new_query)
#             # print(len(self.data))
#             label.extend(new_label)
#     return data, label


class MyDataset(Dataset):
    # def __init__(self, data, label, tokenizer, max_len=512):
    #     self.data = data
    #     self.label = label
    #     self.tokenizer = tokenizer
    #     self.max_len = max_len

    def __init__(self, file_path, tokenizer, max_len=512):
        self.data = []
        self.label = []
        self.tokenizer = tokenizer
        self.max_len = max_len

        # 从JSONL文件中读取数据
        with open(file_path, 'r',encoding='utf-8') as file:
            for line in file:
                new_label = []
                new_query = []
                json_obj = json.loads(line)
                json_instance = json_query_complete(json_obj)
                transform_binary(json_instance, new_label, new_query)
                # print(len(new_query))
                # print(new_label)
                self.data.extend(new_query)
                # print(len(self.data))
                self.label.extend(new_label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text = self.data[index]
        label = self.label[index]
        # print(text)
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,  # 根据需求设置最大长度
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze()
        }, label


"""# training"""

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from torch.optim import AdamW
from torch.utils.data import DataLoader
import torch.nn as nn

from sklearn.metrics import accuracy_score, f1_score, recall_score
import numpy as np


def train(model, train_loader, optimizer, loss_fn, device):
    model.to(device)
    model.train()  # set model to training mode.

    total_loss = 0
    y_true = []
    y_pred = []

    for inputs, labels in train_loader:
        # inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)

        # Forward pass
        outputs = model(input_ids, attention_mask=attention_mask)
        # logits = outputs.logits.squeeze()
        logits = outputs.logits
        # Compute the loss

        loss = loss_fn(logits, labels)
        total_loss += loss.item()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Collect predictions and true labels for evaluation
        # predicted_labels = torch.sigmoid(logits) > 0.5
        predicted_labels = torch.argmax(logits, dim=1)
        y_true.extend(labels.tolist())
        y_pred.extend(predicted_labels.tolist())

        if len(y_true) % 500 == 0:
            print(str(len(y_true)) + "samples, Loss:" + str(loss))

        # Calculate metrics for the epoch

    accuracy = accuracy_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    avg_loss = total_loss / len(train_loader)
    return accuracy, recall, f1, avg_loss


def evaluate(model, data_loader, device):
    model.to(device)
    model.eval()
    y_true = []
    y_pred = []
    # opts_true =[]
    # opts_pred = []
    nc = 0
    ns = 0

    with torch.no_grad():
        for inputs, labels in data_loader:
            # print(labels)
            labels = labels.to(device)
            input_ids = inputs["input_ids"].to(device)
            attention_mask = inputs["attention_mask"].to(device)

            # Forward pass
            outputs = model(input_ids, attention_mask=attention_mask)

            logits = outputs.logits
            predicted_labels = torch.argmax(logits, dim=1)
            y_true.extend(labels.tolist())
            y_pred.extend(predicted_labels.tolist())

            ns += len(labels) / 5
            nc += (labels.argmax() == predicted_labels.argmax())

        accuracy = accuracy_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1= f1_score(y_true, y_pred)
        correctness = nc / ns

        return accuracy, recall, f1, correctness


def train_model(model, train_loader, val_loader, test_loader, test_loader_another, optimizer, loss_fn, device, num_epochs=10):
    # best_val_accuracy = 0.0

    best_val_corr = 0.0
    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        print("-" * 10)

        # Training
        train_accuracy, train_recall, train_f1, train_loss = train(model, train_loader, optimizer, loss_fn, device)
        print(
            f"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}| Train F1: {train_f1:.4f}| \
            Train Recall: {train_recall:.4f}")
        wandb.log({"Train Loss": train_loss, "Train Acc": train_accuracy, "Train F1": train_f1,"Train Recall":\
            train_recall})

        # Validation
        val_accuracy, val_recall, val_f1, val_corr = evaluate(model, val_loader, device)

        print(
            f"Val Correctness: {val_corr:.4f} | val Accuracy: {val_accuracy:.4f}| val F1: {val_f1:.4f}| \
                    val Recall: {val_recall:.4f}")
        wandb.log({"val correctenss": val_corr, "val Acc": val_accuracy, "val F1": val_f1, "val Recall": \
            val_recall})


        # Check if the current model has the best validation accuracy
        # if val_accuracy > best_val_accuracy:
        #     best_val_accuracy = val_accuracy
        if val_corr > best_val_corr:
            best_val_corr = val_corr
            torch.save(model.state_dict(), "best_model.pt")
            print("Best model saved!")

        print()

    print("Training complete.")

    # Load the best model and evaluate on the test set
    model.load_state_dict(torch.load("best_model.pt"))
    test_accuracy, test_recall, test_f1, test_corr = evaluate(model, test_loader, device)
    # print(f"Test Accuracy: {test_accuracy:.4f}")

    # print(f"Test Accuracy: {test_accuracy:.4f} | Test Correctness:{test_corr:.4f}")
    # wandb.log({"Test Acc": test_accuracy, "Test Correctness": test_corr})
    print(
        f"test Correctness: {test_corr:.4f} | test Accuracy: {test_accuracy:.4f}| test F1: {test_f1:.4f}| \
                        test Recall: {test_recall:.4f}")
    wandb.log({"test correctenss": test_corr, "test Acc": test_accuracy, "test F1": test_f1, "test Recall": \
        test_recall})

    print("test on another data set")

    test_accuracy_2, test_recall_2, test_f1_2, test_corr_2 = evaluate(model, test_loader_another, device)
    # print(f"Test Accuracy: {test_accuracy:.4f}")

    # print(f"Test Accuracy: {test_accuracy:.4f} | Test Correctness:{test_corr:.4f}")
    # wandb.log({"Test Acc": test_accuracy, "Test Correctness": test_corr})
    print(
        f"test Correctness 2: {test_corr_2:.4f} | test Accuracy 2: {test_accuracy_2:.4f}| test F1 2: {test_f1_2:.4f}| \
                            test Recall 2: {test_recall_2:.4f}")
    wandb.log({"test correctenss 2": test_corr_2, "test Acc 2": test_accuracy_2, "test F1 2": test_f1_2, "test Recall 2": \
        test_recall_2})


def compute_class_weights(labels):
    class_counts = np.bincount(labels)
    total_samples = len(labels)
    class_weights = total_samples / (len(class_counts) * class_counts)
    return class_weights

def get_data_path(task_id):
    if task_id == 0:
        train_val_json_path = ("../data/training_data/train_test.jsonl")
        test_json_path = ("../data/training_data/train_test.jsonl")
    elif task_id == 1:
        train_val_json_path = ("../data/training_data/Task_1_train.jsonl")
        test_json_path = ("../data/training_data/Task_1_dev.jsonl")

    elif task_id == 2:
        train_val_json_path = ("../data/training_data/Task_2_train.jsonl")
        test_json_path = ("../data/training_data/Task_2_dev.jsonl")

    elif task_id == 3:
        train_val_json_path = ("../data/training_data/Task_1_train.jsonl")
        test_json_path = ("../data/training_data/Task_2_dev.jsonl")

    elif task_id == 4:
        train_val_json_path = ("../data/training_data/Task_2_train.jsonl")
        test_json_path = ("../data/training_data/Task_1_dev.jsonl")
    else:
        print('Wrong task id, task id should be 1, 2, 3')
        return
    return train_val_json_path, test_json_path

def get_newdata_path(task_id):
    if task_id == 0:
        train_json_path = ("../data/training_data/train_test.jsonl")
        val_json_path = ("../data/training_data/train_test.jsonl")
        test_json_path = ("../data/training_data/train_test.jsonl")
        test_another_json_path = ("../data/training_data/train_test.jsonl")
    elif task_id == 1:
        train_json_path = ('../newdata/train/Task_1_train.jsonl')
        val_json_path = ('../newdata/val/Task_1_val.jsonl')
        test_json_path = ('../newdata/test/Task_1_test.jsonl')
        test_another_json_path = ('../newdata/test/Task_2_test.jsonl')

    elif task_id == 2:
        train_json_path = ('../newdata/train/Task_2_train.jsonl')
        val_json_path = ('../newdata/val/Task_2_val.jsonl')
        test_json_path = ('../newdata/test/Task_2_test.jsonl')
        test_another_json_path = ('../newdata/test/Task_1_test.jsonl')
    else:
        print('Wrong task id, task id should be 1, 2')
        return
    return train_json_path,val_json_path, test_json_path, test_another_json_path

def kfold_tune(k=5,train_batch_size=50, learning_rate=1e-5, num_epochs=10, checkpoint="roberta-base", max_len=20,task_id=1):
    # ############### loading data
    # train_json_path = ("../data/training_data/train_test.jsonl")
    # val_json_path = ("../data/training_data/train_test.jsonl")
    # test_json_path = ("../data/training_data/train_test.jsonl")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print('device:', device)
    tokenizer = RobertaTokenizer.from_pretrained(checkpoint)
    model = RobertaForSequenceClassification.from_pretrained(checkpoint)

    train_val_json_path, test_json_path = get_data_path(task_id=task_id)

    train_val_dataset = MyDataset(train_val_json_path, tokenizer, max_len=max_len)
    # test_loader = DataLoader(test_binary_dataset, batch_size=5, shuffle=False)

    test_binary_dataset = MyDataset(test_json_path, tokenizer, max_len=max_len)
    test_loader = DataLoader(test_binary_dataset, batch_size=5, shuffle=False)

    kfold = KFold(n_splits=k, shuffle=False)

    for fold, (train_index, val_index) in enumerate(kfold.split(train_val_dataset)):
        print('Fold ', fold)

        train_sampler = SequentialSampler(train_index)
        val_sampler = SequentialSampler(val_index)

        # 创建 DataLoader 对象
        train_loader = DataLoader(train_val_dataset, batch_size=train_batch_size, sampler=train_sampler)
        val_loader = DataLoader(train_val_dataset, batch_size=5, sampler=val_sampler)

        optimizer = AdamW(model.parameters(), lr=learning_rate)
        # weights = torch.tensor([1.0, 4.0])
        weights = torch.tensor(compute_class_weights(train_val_dataset.label)).float().to(device)
        # print(weights)
        loss_fn = nn.CrossEntropyLoss(weight=weights)

        train_model(model, train_loader, val_loader, test_loader, optimizer, loss_fn, device, num_epochs=num_epochs)



def tune(train_batch_size=50, learning_rate=1e-5, num_epochs=10, checkpoint="roberta-base", max_len=512,task_id=1):
    # ############### loading data
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print('device:', device)
    tokenizer = RobertaTokenizer.from_pretrained(checkpoint)
    model = RobertaForSequenceClassification.from_pretrained(checkpoint)

    train_json_path, val_json_path, test_json_path , test_another_json_path= get_newdata_path(task_id)

    train_set = MyDataset(train_json_path, tokenizer, max_len=max_len)
    val_set = MyDataset(val_json_path, tokenizer, max_len=max_len)
    test_set = MyDataset(test_json_path, tokenizer, max_len=max_len)
    another_test_set =MyDataset( test_another_json_path, tokenizer, max_len=max_len)

    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=False)
    val_loader = DataLoader(val_set, batch_size=5, shuffle=False)
    test_loader = DataLoader(test_set, batch_size=5, shuffle=False)
    another_test_loader = DataLoader(another_test_set, batch_size=5, shuffle=False)

    optimizer = AdamW(model.parameters(), lr=learning_rate)

    # weights = torch.tensor(compute_class_weights(train_val_dataset.label)).float().to(device)
    # loss_fn = nn.CrossEntropyLoss(weight=weights)

    loss_fn = nn.CrossEntropyLoss()
    train_model(model, train_loader, val_loader, test_loader, another_test_loader, optimizer, loss_fn, device, num_epochs=num_epochs)


"""# main"""

if __name__ == '__main__':
    # print(torch.cuda.is_available())
    # device_id = 0 if torch.cuda.is_available() else 'cpu'  # Equivalent to device_id = 'cuda:0'
    # device = torch.device(device_id)  # use these semantics to specify a specific device.

    wandb.login()
    parser = argparse.ArgumentParser(description='Hyper Prm Setting')
    parser.add_argument("-b", "--batch_size", type=int, default=50)
    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-5)
    parser.add_argument("-e", "--num_epochs", type=int, default=10)
    parser.add_argument("-c", "--checkpoint", type=str, default="roberta-base")
    parser.add_argument("-l", "--max_len", type=int, default=512)
    parser.add_argument("-k", "--kfold", type=int, default=5)
    parser.add_argument("-t", "--task", type=int, default=1)

    # parser.add_argument("-b", "--batchsize", type=int)

    args = parser.parse_args()
    train_batch_size, learning_rate, num_epochs, checkpoint, max_len,k,task_id= \
        args.batch_size, args.learning_rate, args.num_epochs, args.checkpoint, args.max_len,args.kfold,args.task

    # train_batch_size = 50
    # learning_rate = 1e-5
    # num_epochs = 10
    # checkpoint = "roberta-base"
    # max_len = 512

    # ######################### hyper prm setting
    run = wandb.init(
        # settings=wandb.Settings(start_method="fork"),
        settings=wandb.Settings(start_method="thread"),
        # Set the project where this run will be logged
        # project="roberta_discriminate+task"+str(task_id),
        project="test",
        # Track hyperparameters and run metadata
        config={
            "learning_rate": learning_rate,
            "epochs": num_epochs,
            "batch_size": train_batch_size,
            "max_len": max_len,
            "checkpoint": checkpoint,
            "kfold":k

        })

    # ####################
    # kfold_tune(k,train_batch_size, learning_rate, num_epochs, checkpoint, max_len,task_id)
    tune(train_batch_size, learning_rate, num_epochs, checkpoint, max_len, task_id)
